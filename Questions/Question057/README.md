Question057 - Ollama 中 Gemma3 模型版本與量化比較差異 ?
=====
* ### 模型命名規則
    * ### ```gemma3:<size>-it``` → Instruction-tuned 模型，適合對話與 Agent 應用。
    * ### 尾碼含義
        * ### ```q4_K_M``` → 4-bit 後量化 (PTQ)，最常見、省顯存但精度下降。
        * ### ```qat``` → Quantization-Aware Training (QAT)，訓練時考慮量化，4-bit 下仍接近原始品質。
        * ### ```q8_0``` → 8-bit 量化 (PTQ)，精度高，接近 FP16，但顯存需求較大。
        * ### ```fp16``` → 半精度浮點，最接近原始模型品質，需最大顯存。
    * ### 備註: ```gemma3:12b``` = ```gemma3:12b-it-q4_K_M```。
* ### 智慧程度排名
    1. gemma3:12b-it-qat (8.9 GB)
    2. gemma3:12b-it-q4_K_M (8.1 GB)
    3. gemma3:4b-it-fp16 (8.6 GB)
    4. gemma3:4b-it-q8_0 (5.0 GB)
    5. gemma3:4b-it-qat (4.0 GB)
    6. gemma3:4b-it-q4_K_M (3.3 GB)
* ### 量化方式對照表
| 量化方式 | 說明 | 優點 | 缺點 |
| - | - | - | - |
| **FP16** | 半精度浮點 | 最接近原始模型，品質最佳 | 顯存需求最高，速度慢 |
| **Q8\_0** | 8-bit 後量化 | 精度高，接近 FP16 | 模型較大，速度略慢 |
| **Q4\_K\_M** | 4-bit 後量化 | 模型小，推理快，顯存省 | 精度下降，較「笨」 |
| **QAT** | 4-bit 感知訓練 | 精度接近原始模型，顯存低 | 訓練成本高，推理略慢 |
<br />

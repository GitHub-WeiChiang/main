from fastapi import APIRouter
from pydantic import BaseModel
import numpy as np
from faiss_index import faiss_handler
from llm import llm_handler
from config import config
from rag_database import rag_handler

router = APIRouter()

class QuestionRequest(BaseModel):
    question: str
    top_k: int = 5
    temperature: float = 0.7

@router.post("/ask")
async def ask_question(request: QuestionRequest):
    """è™•ç†å‰ç«¯çš„å•ç­”è«‹æ±‚ï¼Œé€é FAISS æˆ– RAG æª¢ç´¢æœ€ç›¸é—œçš„å›æ‡‰"""

    # è¨˜éŒ„è«‹æ±‚å…§å®¹
    print(f"ğŸ”¹ [API] æ”¶åˆ°è«‹æ±‚: {request.question}, Top-K: {request.top_k}, Temp: {request.temperature}")

    # ç”¢ç”ŸæŸ¥è©¢å‘é‡
    question_vector = np.array([llm_handler.embed_text(request.question)])

    # å…ˆå˜—è©¦åŒ¹é… FAISS å›ºå®šå›è¦† & å½ˆæ€§å›è¦†
    fixed_distance, fixed_index = faiss_handler.search_fixed(question_vector)
    elastic_distance, elastic_index = faiss_handler.search_elastic(question_vector)

    if fixed_distance is not None and elastic_distance is not None:
        if fixed_distance < elastic_distance and fixed_distance < faiss_handler.threshold:
            matched_key = faiss_handler.fixed_keys[fixed_index]
            print(f"[API] å›ºå®šå›è¦†åŒ¹é…: {matched_key}, è·é›¢: {fixed_distance}")
            return {"answer": config["å›ºå®šå›è¦†"][matched_key], "source": "Cache"}
        elif elastic_distance < faiss_handler.threshold:
            matched_key = faiss_handler.elastic_keys[elastic_index]
            prompt = config["å½ˆæ€§å›è¦†"][matched_key]
            print(f"[API] å½ˆæ€§å›è¦†åŒ¹é…: {matched_key}, è·é›¢: {elastic_distance}")
            response = llm_handler.generate_response(f"{prompt}\n\n{request.question}", request.temperature)
            return {"answer": response, "source": "CAG"}

    if fixed_distance is not None and fixed_distance < faiss_handler.threshold:
        matched_key = faiss_handler.fixed_keys[fixed_index]
        print(f"[API] å›ºå®šå›è¦†åŒ¹é…: {matched_key}, è·é›¢: {fixed_distance}")
        return {"answer": config["å›ºå®šå›è¦†"][matched_key], "source": "Cache"}

    if elastic_distance is not None and elastic_distance < faiss_handler.threshold:
        matched_key = faiss_handler.elastic_keys[elastic_index]
        prompt = config["å½ˆæ€§å›è¦†"][matched_key]
        print(f"[API] å½ˆæ€§å›è¦†åŒ¹é…: {matched_key}, è·é›¢: {elastic_distance}")
        response = llm_handler.generate_response(f"{prompt}\n\n{request.question}", request.temperature)
        return {"answer": response, "source": "CAG"}

    # æ¯”å°æŒ‡å®šæ“ä½œ (é–‹å•Ÿ/é—œé–‰ç³»çµ±)
    command_distance, command_index = faiss_handler.search_command(question_vector)
    if command_distance is not None and command_distance < faiss_handler.threshold:
        matched_key = faiss_handler.command_keys[command_index]
        print(f"[API] æŒ‡å®šæ“ä½œåŒ¹é…: {matched_key}, è·é›¢: {command_distance}")
        return {"answer": config["æŒ‡å®šæ“ä½œ"][matched_key], "source": "Subsystem"}

    # RAG æª¢ç´¢ï¼ˆæ”¹ç‚ºæ”¯æ´å¤šå€‹ chunkï¼‰
    doc_names, doc_chunks = rag_handler.search_rag(question_vector, request.top_k)
    if doc_names and doc_chunks:
        print(f"[API] RAG å‘½ä¸­: {doc_names}")

        # åˆä½µæ‰€æœ‰ chunk è®“ LLM ä½¿ç”¨
        combined_chunks = "\n\n".join(doc_chunks)

        response = llm_handler.generate_response(f"{combined_chunks}\n\n{request.question}", request.temperature)

        return {
            "answer": response,
            "source": "RAG",
            "doc_names": doc_names,  # å›å‚³å¤šå€‹æ–‡ä»¶åç¨±
            "doc_chunks": doc_chunks  # å›å‚³å¤šå€‹ chunk å…§å®¹
        }

    # è‹¥ç„¡åŒ¹é…å‰‡å›å‚³æœªçŸ¥
    return {"answer": "å°šç„¡æ³•å›ç­”æ­¤å•é¡Œï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", "source": "Unknown"}

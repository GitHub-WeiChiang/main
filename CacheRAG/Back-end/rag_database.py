import os
import glob
import datetime
import psycopg2
from langchain_community.document_loaders import Docx2txtLoader  # è®€å– Word æ–‡ä»¶
from langchain.text_splitter import RecursiveCharacterTextSplitter  # æ–‡æœ¬åˆ‡åˆ†
from langchain.embeddings import OllamaEmbeddings  # ç”¢ç”ŸåµŒå…¥å‘é‡
from langchain.vectorstores import PGVector  # é€£æ¥ pgvector

# è¨­å®š pgvector é€£ç·šè³‡è¨Š
PGVECTOR_DB_URL = "postgresql://postgres:postgres@localhost:5432/postgres"

# è¨­å®š Word æ–‡ä»¶å­˜æ”¾çš„è³‡æ–™å¤¾
DOCS_DIR = "doc/"

class RAGDatabase:
    def __init__(self):
        self.embeddings = OllamaEmbeddings(model="nomic-embed-text:latest")  # ç”¢ç”Ÿå‘é‡
        
        # åˆå§‹åŒ– PGVectorï¼Œç¢ºä¿è³‡æ–™è¡¨å­˜åœ¨
        self.vector_store = PGVector(embedding_function=self.embeddings, connection_string=PGVECTOR_DB_URL)
        
        # å•Ÿå‹•æ™‚è‡ªå‹•è™•ç† Word æ–‡ä»¶
        self.load_documents_from_directory()

    def load_documents_from_directory(self):
        """ è‡ªå‹•è®€å– doc/ å…§çš„æ‰€æœ‰ Word æ–‡ä»¶ï¼Œå‘é‡åŒ–ä¸¦å­˜å…¥ pgvector """
        word_files = glob.glob(os.path.join(DOCS_DIR, "*.docx"))  # ç²å– doc/ å…§æ‰€æœ‰ Word æ–‡ä»¶

        if not word_files:
            print("âš ï¸ [RAG] æ²’æœ‰æ‰¾åˆ° Word æ–‡ä»¶ï¼Œè«‹ç¢ºèª doc/ å…§æ˜¯å¦æœ‰å¯è™•ç†çš„æª”æ¡ˆã€‚")
            return

        for file_path in word_files:
            self.load_and_store_document(file_path)

    def load_and_store_document(self, file_path):
        """ è®€å–å–®å€‹ Word æ–‡ä»¶ä¸¦å­˜å…¥å‘é‡è³‡æ–™åº« """
        doc_name = os.path.basename(file_path)  # å–å¾—æ–‡ä»¶åç¨±
        last_modified_time = os.path.getmtime(file_path)  # å–å¾—æª”æ¡ˆæœ€å¾Œä¿®æ”¹æ™‚é–“
        last_modified_str = datetime.datetime.fromtimestamp(last_modified_time).isoformat()

        print(f"ğŸ“‚ [RAG] æ­£åœ¨è™•ç†æ–‡ä»¶: {doc_name}")

        # è®€å–æ–‡ä»¶
        loader = Docx2txtLoader(file_path)
        documents = loader.load()

        # åˆ†å‰²æ–‡æœ¬ç‚º chunk
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        split_docs = text_splitter.split_documents(documents)

        # **ç¢ºä¿æ¯å€‹ chunk éƒ½æœ‰ metadata**
        for chunk in split_docs:
            chunk.metadata = {"source": doc_name, "last_modified": last_modified_str}  # å­˜å…¥ä¿®æ”¹æ™‚é–“

        # **æª¢æŸ¥å‘é‡è³‡æ–™åº«æ˜¯å¦å·²ç¶“æœ‰ç›¸åŒçš„æ–‡ä»¶**
        if self.document_exists(doc_name, last_modified_str):
            print(f"âœ… [RAG] {doc_name} å·²å­˜åœ¨æ–¼å‘é‡è³‡æ–™åº«ï¼Œä¸”ä¿®æ”¹æ™‚é–“æœªè®Šæ›´ï¼Œè·³éå­˜å…¥ã€‚")
            return

        # å­˜å…¥å‘é‡è³‡æ–™åº«
        self.vector_store.add_documents(split_docs)
        print(f"âœ… [RAG] {doc_name} å·²æˆåŠŸå­˜å…¥å‘é‡è³‡æ–™åº«ï¼Œå…± {len(split_docs)} å€‹ chunkã€‚")

    def document_exists(self, doc_name, last_modified_str):
        """ ä½¿ç”¨ SQL æŸ¥è©¢ pgvector è³‡æ–™åº«ä¸­çš„ metadata ä¾†åˆ¤æ–·æ˜¯å¦æœ‰ç›¸åŒçš„æª”æ¡ˆåç¨±èˆ‡ä¿®æ”¹æ™‚é–“ """
        try:
            conn = psycopg2.connect(PGVECTOR_DB_URL)
            cursor = conn.cursor()
            
            query = """
            SELECT cmetadata->>'last_modified'
            FROM langchain_pg_embedding
            WHERE cmetadata->>'source' = %s
            LIMIT 1;
            """
            cursor.execute(query, (doc_name,))
            result = cursor.fetchone()
            
            cursor.close()
            conn.close()

            if result:
                existing_time = result[0]
                if existing_time == last_modified_str:
                    return True  # è‹¥ä¿®æ”¹æ™‚é–“ç›¸åŒï¼Œè¦–ç‚ºå·²å­˜åœ¨
        except Exception as e:
            print(f"âŒ [RAG] æŸ¥è©¢å‘é‡è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        return False

    def search_rag(self, query_text, top_k=3):
        """ åœ¨ pgvector è³‡æ–™åº«ä¸­æœå°‹æœ€ç›¸é—œçš„ chunkï¼Œè¿”å›å¤šå€‹åŒ¹é…çµæœ """
        retriever = self.vector_store.as_retriever(search_kwargs={"k": top_k})
        docs = retriever.get_relevant_documents(query_text)

        if docs:
            doc_names = [doc.metadata.get("source", "æœªçŸ¥æ–‡ä»¶") for doc in docs]  # å–å¾—æ‰€æœ‰æ–‡ä»¶åç¨±
            doc_chunks = [doc.page_content for doc in docs]  # å–å¾—æ‰€æœ‰ chunk å…§å®¹
            return doc_names, doc_chunks
        return [], []

# åˆå§‹åŒ– RAG æ¨¡çµ„ï¼Œç¢ºä¿ FastAPI å•Ÿå‹•æ™‚åŠ è¼‰æ–‡ä»¶
rag_handler = RAGDatabase()

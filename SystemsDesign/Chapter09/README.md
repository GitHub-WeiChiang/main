Chapter09 設計網路爬蟲
=====
* ### 需要一組起始網址，再根據這些網頁內的鏈結，去收集更多的網址。
* ### Google 搜尋引擎透過爬蟲抓取資訊，解析這些頁面中的程式碼，並把其中可作為標記的程式碼片段，建立索引，儲存在數據庫當中，待用戶搜尋時再抽取使用。
* ### 網路爬蟲用途
    * ### 搜尋引擎索引編製: 建立搜尋引擎的本地索引。
    * ### 網路內容歸檔: 收集資訊並保存，例如網站封存。
    * ### 網路探勘: 就是抓資訊啦。
    * ### 網路監控: 監控版權與商標侵權情形。
* ### 步驟
    * ### 給定一組網址，並進行下載。
    * ### 根據這些網頁提出內部網址。
    * ### 將新網址放入下載列表，並重複上述動作。
* ### 高階設計
    * ### 種子網址 (sed url): 搜尋起點，可以透過地區或是主題劃分。
    * ### 網址邊境 (url frontier): 將擷取狀態分為要下載與已下載。
    * ### HTML 下載器: 負責下載網頁。
    * ### 內容解析器: 針對內容進行解析與驗證，通常這會是獨立伺服器處理。
    * ### 看過的內容 ?: 判斷內容是否看過，消除冗餘。
    * ### 內容儲存系統 (Content Storage): 用於儲存 HTML 內容，大部分存於磁碟，常用存快取。
    * ### 網址鏈結提取器: 解析 HTML 內容並提出其中網址鏈結。
    * ### 網址篩選器: 排除某些內容類型、檔案格式與黑名單過濾。
    * ### 看過的網址 ?: 避免添加相同網址，可以透過 Bloom 篩選器實作，沒看過的丟回第二步驟重複執行。
    * ### 網址儲存系統: 儲存已造訪網址。
* ### 深入設計
    * ### 深度優先: 不建議，太深了。
    * ### 廣度優先: 通常採用，並搭配 FIFO，亦可以根據網頁排名、流量、更新頻率與官方、非官方網站等作為優先順序排列依據。
    * ### 網址邊境: 保存需要下載網址，並確保禮貌性 (可以透過同一個網域一次只下載一份頁面或下載間隔延遲達成)。
    * ### 佇列路由器: 透過網域進行下載分類。
* ### 內容新鮮度: 網頁若有更新需要重新爬取。
* ### 機器人排除協定 (Robots Exclusion Protocol): robots.txt 內包含不允許下載的類型。
* ### 效能最佳化
    * ### 分散式資料擷取。
    * ### 快取 DNS 解析器 (自行維護一個 DNS 並定期更新)。
    * ### 地理性考量 (依據地理位置指定執行爬蟲任務的主機)。
    * ### 短一點的超時時間 (設定 timeout)。
    * ### 穩健性 (一致性雜湊、保存資料擷取狀態、異常處理、資料驗證)。
* ### 需要注意
    * ### 多餘內容 (重複)。
    * ### 網路蜘蛛陷阱 (迴圈)。
    * ### 資料雜訊 (廣告)。
* ### ![image](https://raw.githubusercontent.com/GitHub-WeiChiang/main/master/SystemsDesign/Chapter09/SystemArchitectureDiagram.png)
<br />
